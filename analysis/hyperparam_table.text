\begin{tabular}{rlrrrrrrrlrlrrr}
\toprule
    loss & optimiser &  batch\_size &  num\_epochs &  learning\_rate &  weight\_decay &  momentum &  num\_hidden &  hidden\_size & activation &  dropout\_rate &  batch\_normalisation &  accuracy &  f1\_macro &  combined\_metric \\
\midrule
0.129472 &       sgd &          64 &         100 &           0.01 &        0.0001 &       0.5 &           2 &          128 &       relu &           0.0 &                False &   0.54322 &  0.539458 &         3.697430 \\
0.130119 &       sgd &          64 &         100 &           0.01 &        0.0001 &       0.5 &           4 &          128 &       relu &           0.0 &                False &   0.54296 &  0.539469 &         3.672541 \\
0.129652 &       sgd &          64 &         100 &           0.01 &        0.0001 &       0.5 &           3 &          128 &       relu &           0.0 &                False &   0.54226 &  0.538225 &         3.668001 \\
0.130089 &       sgd &         256 &         100 &           0.10 &        0.0010 &       0.5 &           4 &          128 &       relu &           0.0 &                False &   0.53612 &  0.533339 &         3.533703 \\
0.130862 &       sgd &         256 &         100 &           0.10 &        0.0010 &       0.5 &           2 &          128 &       relu &           0.0 &                False &   0.53528 &  0.531903 &         3.483601 \\
0.130459 &       sgd &         256 &         100 &           0.10 &        0.0010 &       0.5 &           3 &          128 &       relu &           0.0 &                False &   0.53412 &  0.529010 &         3.455354 \\
0.131890 &       sgd &          64 &          40 &           0.01 &        0.0001 &       0.5 &           4 &          128 &       relu &           0.0 &                False &   0.53038 &  0.527503 &         3.348459 \\
0.133533 &       sgd &         256 &          40 &           0.10 &        0.0010 &       0.9 &           2 &          128 &       relu &           0.0 &                 True &   0.52904 &  0.525766 &         3.260070 \\
0.134093 &       sgd &         256 &          40 &           0.01 &        0.0010 &       0.9 &           4 &          128 &       relu &           0.0 &                 True &   0.52904 &  0.526589 &         3.249157 \\
0.132554 &       sgd &          64 &         100 &           0.01 &        0.0001 &       0.5 &           3 &           64 &       relu &           0.0 &                False &   0.52730 &  0.523107 &         3.246882 \\
0.133356 &       sgd &         256 &         100 &           0.01 &        0.0001 &       0.5 &           3 &          128 &       relu &           0.0 &                False &   0.52710 &  0.523382 &         3.220053 \\
0.133068 &       sgd &          64 &          40 &           0.01 &        0.0001 &       0.5 &           3 &          128 &       relu &           0.0 &                False &   0.52752 &  0.521648 &         3.217686 \\
0.134000 &       sgd &         256 &         100 &           0.01 &        0.0001 &       0.5 &           4 &          128 &       relu &           0.0 &                False &   0.52792 &  0.523800 &         3.211882 \\
0.133554 &       sgd &          64 &          40 &           0.01 &        0.0001 &       0.5 &           2 &          128 &       relu &           0.0 &                False &   0.52682 &  0.523004 &         3.206361 \\
0.134221 &       sgd &         256 &         100 &           0.01 &        0.0010 &       0.9 &           4 &          128 &       relu &           0.0 &                 True &   0.52662 &  0.525329 &         3.204252 \\
\bottomrule
\end{tabular}
